\documentclass[conference]{IEEEtran}

%-----------------Hyperlink Packages--------------------
\usepackage{hyperref}
\hypersetup{
	 colorlinks   = true,
     citecolor    = black,
     linkcolor    = black,
     urlcolor     = black
}
%-----------------Figure Packages--------------------
\usepackage{graphicx}                       % For figures
\usepackage{stfloats}
\usepackage[tight,footnotesize]{subfigure}  % Create subfigures, ie 1A, 1B
%\usepackage{epsfig} % for postscript graphics files
%------------------Math Packages------------------------
\usepackage{amssymb,amsmath}
\usepackage{textcomp}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}
%------------------Table Packages-----------------------
\usepackage{rotating}                     % Used to rotate tables
\usepackage{array}                        % Fixed column widths for tables
%-----------------Algorithm Packages--------------------
\usepackage{listings}                     % Source code
\usepackage{algorithm}                    % Pseudo Code
\usepackage[noend]{algpseudocode}
%---------------------------------------------------------
\title{\LARGE \bf A Survey on Data Clustering}
\author{
Qiuyi Zhang \\
School of Software \\
Sun Yat-Sen University \\
Guangzhou, Guangdong Province, China \\
{\tt \href{mailto:zhangqy37@mail2.sysu.edu.cn}{zhangqy37@mail2.sysu.edu.cn}}
}
%---------------------------------------------------------
\begin{document}
\maketitle
%-------------------Abstract and keywords-----------------
\begin{abstract}
Discovering closely related groups in the data set is a prevalent topic in data analysis. Data clustering is the technique of finding natural groups, or clusters, in a data set of patterns, points, or objects. In this paper, we present an overview of well-known clustering algorithms, review common methods for evaluating clustering algorithms, and describe some important applications of data clustering.
\end{abstract}

\begin{IEEEkeywords}
Data Clustering, Survey, Applications, Cluster Evaluation
\end{IEEEkeywords}
%----------------------Introduction-------------------------
\section{INTRODUCTION}\label{sec:Introduction}
\subsection{Background}
Development in computer science and software engineering has made it possible for huge amount of data to be generated and stored. The popularity of the Internet also leads to the creation and sharing of numerous high-volume data sets. The dramatic increase in the available data calls for effective methods to organize, analyze, and summarize data. 

\subsection{What is data clustering?}
According to the Merriam-Webster dictionary \cite{webster2006merriam}, cluster analysis is ``a statistical classification technique for discovering whether the individuals of a population fall into different groups by making quantitative comparisons of multiple characteristics.'' In short, the goal of data clustering is to find natural groupings in the data set. 

\subsection{Development}
As a commonly used technique in computer science, data clustering also has a long and rich history in other areas such as biology, medical research, geology, and marketing. This explains the considerable number of clustering algorithms and their evaluation studies available in the literature. In this paper, we introduce three of the most important clustering algorithms: \textit{hierarchical clustering}, \textit{partitional clustering}, and \textit{density-based clustering}. We then review three criteria for evaluating clustering algorithms: \textit{external criteria}, \textit{internal criteria}, and \textit{relative criteria}.

\subsection{Application}
Although data clustering has a wide application, we will focus on its application in computer science. Clustering has been used in:
\begin{itemize}
\item \textbf{Image Processing}: to partition a digital image into multiple segments, improve the quality of color reduction, and achieve better image compression.
\item \textbf{Object and Feature Recognition}: to identify object, detect anomalies, and recognize feature in data.
\item \textbf{Information Retrieval}: to classify documents, improve search results, and detect similarity between documents.
\end{itemize}

%----------------------------------------------------------
\section{DATA CLUSTERING ALGORITHMS}
\subsection{Hierarchical Clustering}
Hierarchical clustering first organizes data objects into clusters, and then repeatedly merges these clusters into larger ones, until a desired hierarchy is generated. The representation of this hierarchy is usually a tree, called \textit{dendrogram}, with data objects being the leaves, nonempty clusters being the interior nodes. This method enables the exploration of data at different levels of granularity.

Although hierarchical clustering is one of the theoretical foundations of data clustering, some consider it obsolete \cite{chowdary2014evaluating}. It did, however, provide inspiration for later methods such as density based clustering. Numerous efforts have been put into its improvement, one of them is the CURE algorithm \cite{guha1998cure}.

The CURE algorithm starts with choosing $c$ well-scattered points in each cluster (initially $c = 1$ i.e. each point is a cluster itself). Then, these points will be shrunk towards the center of the cluster by a fraction $\alpha$ to determine $c$ representative points in each cluster. At each step of CURE's hierarchical clustering algorithm, clusters with the closest pair of representatives will be merged. In this way, CURE correctly identifies the clusters while being less sensitive to outliers.

Assume the input size is $n$, the space complexity of CURE is $O(n)$. The worst-case time complexity is $O(n^{2}\log n)$, which can be further reduced to $O(n^{2})$ using sampling, partitioning and labeling as optimizations \cite{gan2007data}. Therefore, CURE, unlike those less-efficient hierarchical algorithms, can be used in large databases.

\subsection{Partitional Clustering}
Instead of producing a clustering structure, like the tree generated by a hierarchical algorithm, a partitional clustering algorithm outputs the partition of the data. Partitional algorithms have advantages in applications with large data sets, where a hierarchical clustering algorithm may not be efficient enough.

The \textit{k-means algorithm}, which is a type of partitional clustering algorithms, is the most popular clustering algorithm used in scientific and industrial applications \cite{wang2006encyclopedia}. It refers to a family of algorithms for solving the $k-means$ problem. This name, first proposed by James MacQueen in 1967 \cite{macqueen1967some}, comes from representing each of the $k$ clusters $C_{j}$ by the mean (or weighted average) $c_{j}$ of its data points.

When the number of clusters is fixed to $k$, the \textit{k-means problem} is an optimization problem of finding
\begin{enumerate}
\item $k$ cluster centers
\item the objects belongs to each cluster
\end{enumerate}
to minimize the squared distance of each point to its closest \textit{centroid} (the center, or mean, of the cluster).

In mathematical terms, the goal is find:

$$ \underset{\mathbf{S}}{\operatorname{arg\,min}} \sum_{i=1}^{k} \sum_{\mathbf x \in S_i} \left\| \mathbf x - \boldsymbol\mu_i \right\|^2
$$

where $n$ is the number of points, $k$ is the number of clusters, $S = \{S_1, S_2, ... S_k\}$ represents points in each cluster, $\mu_i$ is the mean of points in $S_i$.

The optimization problem is known to be NP-hard \cite{mahajan2009planar}, thus algorithms solving this problem usually search only for approximate solutions. The standard k-means algorithm usually refers to the one proposed by Stuart P. Lloyd \cite{lloyd1982least}.


The popularity of the k-means algorithm comes from its simplicity and efficiency. Its time complexity is $O(nkt)$, with $n$ being the number of objects, $k$ being the number of clusters, and $t$ being the number of iterations \cite{dong2009k}. This algorithm is, however, only capable of finding a local optimum. It needs multiple runs with different initializations to produce a good result.

\subsection{Density-based Clustering}
In density-based clustering, clusters are defined as regions with higher density than the remainder in the data set. The low-density regions surrounding the higher density regions are considered as noise.

One of the most popular density-based clustering algorithms is DBSCAN \cite{ester1996density}. It uses \textit{density-reachability} as its cluster model. In this model, points that meet a density criterion, e.g. a minimum number of points within its radius, are called \textit{density-reachable}. A cluster is the set of all density-connected points plus all points that are \textit{density-reachable}.

The admissible complexity of DBSCAN is one of the reasons why it is so popular. Assume that the average time complexity of a single region query is $O(\log n)$, the complexity of DBSCAN would be just $O(n \log n)$ \cite{ester1996density}. Another reason is that there is no need to run it multiple times to get a better result, since it will find essentially the same clusters every time.

%----------------------------------------------------------
\section{EVALUATION OF CLUSTERING RESULTS} \label{sec:evaluation}

There are three categories of criteria used in the evaluation of clustering results: \textit{external criteria}, \textit{internal criteria}, and \textit{relative criteria}. The first two are computationally expensive because they use statistical testing, while relative criteria, which do not need statistical tests, are more efficient. In this paper, we will give a basic description of these three criteria.

\subsection{External criteria}
When using external criteria, we compare the output of the clustering algorithm with a pre-specified clustering structure. In general, there are two kinds of structure used in external criteria:

\begin{enumerate}
\item a partition built on human intuition
\item a proximity matrix $P$ consisting of distances between each pair of data points
\end{enumerate}

Both approaches use the \textit{Monte Carlo} method (repeat the evaluation multiple times) to compute the probability density function of cluster validity. They are detailedly discussed in \cite{Halkidi:2002:CVM:565117.565124}.

\subsection{Internal criteria}
In the internal criteria approach, we evaluate the output of a algorithm with only quantities and attributes inherited from the original data. There are two situations:

\begin{enumerate}
\item When the output of the algorithm is a hierarchy, we compute the proximity levels of each pair of data points when they are assigned to the same cluster for the first time, and use the proximity levels to construct a \textit{cophenetic matrix} $P_c$. Then we compare $P_c$ with the original proximity matrix $P$ to evaluate the algorithm.
\item When the output of the algorithm is not a hierarchy, we compare a cluster membership matrix to $P$.
\end{enumerate}

The Monte Carlo method is also used in the internal criteria for evaluation. \cite{Halkidi:2002:CVM:565117.565124} and \cite{milligan1981monte} describe the internal criteria in detail.

\subsection{Relative criteria}
The basic idea of these criteria is to run the algorithm with different parameters, use a pre-defined criterion to determine the best clustering result, and then use this result for evaluation. Depending on whether the number of clusters $n_c$ is a parameter, there are two ways to determine the best clustering result.

\begin{enumerate}
\item When $n_c$ is not a parameter, we run the algorithm with a wide range of parameters, and choose the largest range for which $n_c$ remains constant. Then we use the middle of this range as the optimal parameters.
\item When $n_c$ is a parameter, we run the algorithm  with different combinations of other parameters for every possible $n_c$, then use a validity index to choose the best parameter set for each $n_c$. Finally, we choose the best parameters among the previous results.
\end{enumerate}

A more detailed discussion of the relative criteria can be found in \cite{Halkidi:2002:CVC:601858.601862}. \cite{bezdek1998some} compares some validity indices commonly used in this approach.

%----------------------------------------------------------
\section{APPLICATIONS OF DATA CLUSTERING} \label{sec:application}
Data clustering has been widely used in different fields. In the following sections, we describe its application in some disciplines of computer science.

\subsection{Image processing}
In image processing, clustering is a useful tool in image segmentation i.e. dividing a digital image into distinct regions \cite{haralick1985image}. Clustering can help to reduce the color of an image by grouping colors to find representative colors in the image \cite{scheunders1997comparison}. Some image compression methods also use clustering to identify the patterns in the image to achieve better compression \cite{jain1981image}.

\subsection{Object and Feature Recognition}
By grouping object views into classes based on the similarity of shape or spectral features, clustering can improve the recognition of objects and borders in images, audios, or videos. \cite{lowe2004distinctive}. It is also helpful when detecting features and characters in handwritten text \cite{lu1995machine} \cite{yin2009handwritten}.

\subsection{Information Retrieval}
In information retrieval, clustering can be used to classify documents \cite{willett1988recent} \cite{steinbach2000comparison}. Search engines aware of the clusters in the documents can produce better results, and search results can be further grouped to provide effective information representation \cite{cutting1992scatter}. Clustering can also aid in the construction of thesauri to address other issues in information retrieval \cite{lin1998automatic}.

%------------------------Conclusion------------------------
\section{CONCLUSION} \label{sec:Conclusion}

The task of organizing data into meaningful groups is common in various scientific fields, which explains the continued appearance of data clustering in the literature. 

Three of the most common approaches of data clustering are hierarchical clustering, partitional clustering, and density-based clustering. Hierarchical clustering contributed to the early development of cluster analysis, but it is usually too expensive in performance. Partitional clustering algorithms, such as the classic k-means algorithm, are simple and fast, but the quality of their result varies. Density-based clustering is admissible in performance while providing a stable output. These clustering techniques have been widely used in numerous scientific and industrial applications.

There are three categories of evaluation criteria for clustering algorithms: external criteria, internal criteria, and relative criteria. The first two are based on statistical testing, therefore they have high computational costs. Relative criteria do not involve statistical tests, so it is more efficient than the others.

It is, however, important to understand that the output of cluster analysis only suggests hypotheses. For many data sets, there is no \textit{right} clustering. While there are numerous clustering algorithms developed and new ones continue to emerge, no single clustering algorithm can dominate other algorithms across all application areas. Most of the time, it is sensible to choose among the existing clustering methods, or develop a new one to achieve the best results.

%-----------------------Acknowlegement----------------------
\section*{Acknowledgment}

We wish to acknowledge the Sun Yat-Sen University for supporting this work. We are also grateful for the generosity of our colleagues who have read the manuscript drafts, made many suggestions to improve the quality of them, and provided useful materials that we have incorporated into this paper. 

\bibliographystyle{IEEEtran}
\bibliography{clustering}
\end{document}
